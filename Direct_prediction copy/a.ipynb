{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access: ../Data/data_angles/data6_angles.csv\n",
      "Full path: /home/exx/Desktop/quantum/Data/data_angles/data6_angles.csv\n",
      "File found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = \"../Data/data_angles/data6_angles.csv\"\n",
    "print(\"Attempting to access:\", data_path)\n",
    "print(\"Full path:\", os.path.abspath(data_path))\n",
    "if os.path.exists(data_path):\n",
    "    print(\"File found!\")\n",
    "else:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c16cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA H100 NVL\n",
      "Attempting to access: ../Data/data_angles/data6_angles.csv\n",
      "Full path: /home/exx/Desktop/quantum/Data/data_angles/data6_angles.csv\n",
      "File found!\n",
      "\n",
      "Experimenting with window size: 8\n",
      "Normalized Feature Phi range: -0.9987468179464775 to 1.0\n",
      "Normalized Feature Theta range: 0.004026122250999709 to 0.9932257217575585\n",
      "Normalized Target Phi range: -0.9996426482636093 to 0.9999915158988666\n",
      "Normalized Target Theta range: 0.008747641666239578 to 0.9978211921309839\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2149, Val Loss: 0.1229\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1171, Val Loss: 0.1010\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.0997, Val Loss: 0.0865\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0856, Val Loss: 0.0740\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0731, Val Loss: 0.0612\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0643, Val Loss: 0.0567\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0584, Val Loss: 0.0521\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0554, Val Loss: 0.0504\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0525, Val Loss: 0.0491\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0519, Val Loss: 0.0492\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0495, Val Loss: 0.0461\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0485, Val Loss: 0.0448\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0468, Val Loss: 0.0435\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0460, Val Loss: 0.0421\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0448, Val Loss: 0.0408\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0430, Val Loss: 0.0396\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0415, Val Loss: 0.0380\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0407, Val Loss: 0.0378\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0398, Val Loss: 0.0361\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0387, Val Loss: 0.0345\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0375, Val Loss: 0.0335\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0357, Val Loss: 0.0329\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0355, Val Loss: 0.0316\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0357, Val Loss: 0.0315\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0347, Val Loss: 0.0310\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0342, Val Loss: 0.0300\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0342, Val Loss: 0.0306\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0341, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0325, Val Loss: 0.0302\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0331, Val Loss: 0.0291\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0332, Val Loss: 0.0293\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0326, Val Loss: 0.0294\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0328, Val Loss: 0.0294\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0319, Val Loss: 0.0286\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0327, Val Loss: 0.0296\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0319, Val Loss: 0.0287\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0326, Val Loss: 0.0285\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0316, Val Loss: 0.0288\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0316, Val Loss: 0.0284\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0312, Val Loss: 0.0283\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0314, Val Loss: 0.0283\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0323, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0315, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0312, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0315, Val Loss: 0.0283\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0312, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0319, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0320, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0312, Val Loss: 0.0282\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0310, Val Loss: 0.0282\n",
      "\n",
      "Experimenting with window size: 16\n",
      "Normalized Feature Phi range: -0.9987468179464775 to 1.0\n",
      "Normalized Feature Theta range: 0.004026122250999709 to 0.9932257217575585\n",
      "Normalized Target Phi range: -0.9996426482636093 to 0.9999915158988666\n",
      "Normalized Target Theta range: 0.008747641666239578 to 0.9978211921309839\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2774, Val Loss: 0.1162\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1068, Val Loss: 0.0843\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.0833, Val Loss: 0.0675\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0729, Val Loss: 0.0591\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0667, Val Loss: 0.0569\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0627, Val Loss: 0.0540\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0608, Val Loss: 0.0530\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0586, Val Loss: 0.0536\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0573, Val Loss: 0.0510\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0555, Val Loss: 0.0502\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0548, Val Loss: 0.0484\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0530, Val Loss: 0.0484\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0525, Val Loss: 0.0469\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0511, Val Loss: 0.0468\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0499, Val Loss: 0.0463\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0493, Val Loss: 0.0446\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0480, Val Loss: 0.0439\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0472, Val Loss: 0.0418\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0461, Val Loss: 0.0418\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0453, Val Loss: 0.0413\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0443, Val Loss: 0.0390\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0427, Val Loss: 0.0388\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0415, Val Loss: 0.0385\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0411, Val Loss: 0.0361\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0401, Val Loss: 0.0351\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0387, Val Loss: 0.0353\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0381, Val Loss: 0.0335\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0380, Val Loss: 0.0329\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0374, Val Loss: 0.0334\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0369, Val Loss: 0.0322\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0361, Val Loss: 0.0319\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0359, Val Loss: 0.0316\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0358, Val Loss: 0.0321\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0348, Val Loss: 0.0308\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0349, Val Loss: 0.0327\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0349, Val Loss: 0.0311\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0347, Val Loss: 0.0304\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0352, Val Loss: 0.0307\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0341, Val Loss: 0.0303\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0342, Val Loss: 0.0304\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0343, Val Loss: 0.0302\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0346, Val Loss: 0.0297\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0339, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0343, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0337, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0342, Val Loss: 0.0298\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0343, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0336, Val Loss: 0.0300\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0339, Val Loss: 0.0299\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0334, Val Loss: 0.0299\n",
      "\n",
      "Experimenting with window size: 32\n",
      "Normalized Feature Phi range: -0.9987468179464775 to 1.0\n",
      "Normalized Feature Theta range: 0.004026122250999709 to 0.9932257217575585\n",
      "Normalized Target Phi range: -0.9996426482636093 to 0.9999915158988666\n",
      "Normalized Target Theta range: 0.008747641666239578 to 0.9978211921309839\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.3516, Val Loss: 0.1234\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1139, Val Loss: 0.0963\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.0916, Val Loss: 0.0781\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0792, Val Loss: 0.0686\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0723, Val Loss: 0.0633\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0663, Val Loss: 0.0565\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0611, Val Loss: 0.0515\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0569, Val Loss: 0.0479\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0540, Val Loss: 0.0460\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0509, Val Loss: 0.0422\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0481, Val Loss: 0.0397\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0454, Val Loss: 0.0378\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0430, Val Loss: 0.0362\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0414, Val Loss: 0.0346\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0403, Val Loss: 0.0336\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0387, Val Loss: 0.0331\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0372, Val Loss: 0.0318\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0367, Val Loss: 0.0310\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0358, Val Loss: 0.0307\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0348, Val Loss: 0.0297\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0338, Val Loss: 0.0284\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0336, Val Loss: 0.0277\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0330, Val Loss: 0.0270\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0325, Val Loss: 0.0268\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0317, Val Loss: 0.0262\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0313, Val Loss: 0.0260\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0307, Val Loss: 0.0256\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0302, Val Loss: 0.0251\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0302, Val Loss: 0.0249\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0298, Val Loss: 0.0248\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0297, Val Loss: 0.0253\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0297, Val Loss: 0.0245\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0290, Val Loss: 0.0242\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0295, Val Loss: 0.0240\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0293, Val Loss: 0.0239\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0292, Val Loss: 0.0237\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0286, Val Loss: 0.0236\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0288, Val Loss: 0.0236\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0282, Val Loss: 0.0235\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0285, Val Loss: 0.0234\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0281, Val Loss: 0.0234\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0286, Val Loss: 0.0234\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0287, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0278, Val Loss: 0.0233\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0284, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0279, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0280, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0279, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0272, Val Loss: 0.0232\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0283, Val Loss: 0.0232\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Verify GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected, exiting!\")\n",
    "    exit(1)\n",
    "\n",
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, data_path: str, window_size: int):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        if self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                   'Target Phi (degrees)', 'Target Theta (degrees)']].isna().any().any():\n",
    "            print(\"Warning: NaN values found in data!\")\n",
    "            self.df = self.df.dropna()\n",
    "        if np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                            'Target Phi (degrees)', 'Target Theta (degrees)']].values).any():\n",
    "            print(\"Warning: Infinite values found in data!\")\n",
    "            self.df = self.df[~np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                                                'Target Phi (degrees)', 'Target Theta (degrees)']]).any(axis=1)]\n",
    "        \n",
    "        self.features = self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)']].values\n",
    "        self.targets = self.df[['Target Phi (degrees)', 'Target Theta (degrees)']].values\n",
    "        self.features[:, 0] /= 180.0\n",
    "        self.features[:, 1] /= 180.0\n",
    "        self.targets[:, 0] /= 180.0\n",
    "        self.targets[:, 1] /= 180.0\n",
    "        \n",
    "        print(\"Normalized Feature Phi range:\", self.features[:, 0].min(), \"to\", self.features[:, 0].max())\n",
    "        print(\"Normalized Feature Theta range:\", self.features[:, 1].min(), \"to\", self.features[:, 1].max())\n",
    "        print(\"Normalized Target Phi range:\", self.targets[:, 0].min(), \"to\", self.targets[:, 0].max())\n",
    "        print(\"Normalized Target Theta range:\", self.targets[:, 1].min(), \"to\", self.targets[:, 1].max())\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.length = len(self.df) - window_size + 1\n",
    "        self.indices = np.arange(window_size - 1, len(self.df))  # Time indices for targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        window = self.features[idx:idx + self.window_size]\n",
    "        target = self.targets[idx + self.window_size - 1]\n",
    "        index = self.indices[idx]\n",
    "        return torch.FloatTensor(window), torch.FloatTensor(target), index\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scale = 1.0 / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class AnglePredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int, n_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 128, d_model) * 0.1)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            FlashAttention(d_model, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(d_model, 2)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        for attn, norm in zip(self.attn_layers, self.norm_layers):\n",
    "            residual = x\n",
    "            x = attn(x)\n",
    "            x = norm(x + residual)\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        return self.output(x)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                epochs: int, device: torch.device, lr: float = 1e-4):\n",
    "    print(\"Model device:\", next(model.parameters()).device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (batch_x, batch_y, _) in enumerate(train_loader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            if i == 0:\n",
    "                print(\"Batch x device:\", batch_x.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(batch_x)\n",
    "                if torch.isnan(output).any():\n",
    "                    print(f\"NaN in output at epoch {epoch+1}, batch {i}\")\n",
    "                    break\n",
    "                loss = criterion(output, batch_y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, _ in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                val_loss += criterion(output, batch_y).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def experiment_window_sizes(data_path: str, window_sizes: list, batch_size: int = 64, \n",
    "                          epochs: int = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"\\nExperimenting with window size: {window_size}\")\n",
    "        \n",
    "        dataset = AngleDataset(data_path, window_size)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Get validation indices in dataset order\n",
    "        val_indices = np.array([dataset.indices[idx] for idx in val_dataset.indices])\n",
    "        sorted_order = np.argsort(val_indices)\n",
    "        val_indices_sorted = val_indices[sorted_order]\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AnglePredictionModel(\n",
    "            input_dim=2,\n",
    "            d_model=32,  # Reduced for simpler mapping\n",
    "            n_heads=4,\n",
    "            n_layers=2,  # Reduced for stability\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        \n",
    "        train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader, epochs, device\n",
    "        )\n",
    "        \n",
    "        # Generate predictions on validation set\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        time_indices = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_indices in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                output = output.cpu().numpy() * 180.0\n",
    "                batch_y = batch_y.cpu().numpy() * 180.0\n",
    "                predictions.append(output)\n",
    "                actuals.append(batch_y)\n",
    "                time_indices.append(batch_indices.numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        actuals = np.concatenate(actuals, axis=0)\n",
    "        time_indices = np.concatenate(time_indices, axis=0)\n",
    "        \n",
    "        # Sort by time indices to ensure time series order\n",
    "        sort_idx = np.argsort(time_indices)\n",
    "        time_indices = time_indices[sort_idx]\n",
    "        predictions = predictions[sort_idx]\n",
    "        actuals = actuals[sort_idx]\n",
    "        \n",
    "        # Compute RMSE\n",
    "        rmse_phi = np.sqrt(np.mean((predictions[:, 0] - actuals[:, 0])**2))\n",
    "        rmse_theta = np.sqrt(np.mean((predictions[:, 1] - actuals[:, 1])**2))\n",
    "        \n",
    "        # Plot predictions vs actuals as line graphs\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Phi plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(time_indices, actuals[:, 0], label='Actual Phi (1551 nm, °)', color='blue', linewidth=2)\n",
    "        plt.plot(time_indices, predictions[:, 0], label='Predicted Phi (from 1550 nm, °)', color='red', linestyle='--', linewidth=2)\n",
    "        plt.title(f'Phi Time Series (Window Size: {window_size}, RMSE: {rmse_phi:.2f}°)')\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.ylabel('Phi (degrees)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Theta plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(time_indices, actuals[:, 1], label='Actual Theta (1551 nm, °)', color='blue', linewidth=2)\n",
    "        plt.plot(time_indices, predictions[:, 1], label='Predicted Theta (from 1550 nm, °)', color='red', linestyle='--', linewidth=2)\n",
    "        plt.title(f'Theta Time Series (Window Size: {window_size}, RMSE: {rmse_theta:.2f}°)')\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.ylabel('Theta (degrees)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'predictions_window_{window_size}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot absolute errors\n",
    "        errors = np.abs(predictions - actuals)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(time_indices, errors[:, 0], label='|Predicted - Actual| Phi', color='purple', linewidth=2)\n",
    "        plt.title(f'Phi Absolute Error (Window Size: {window_size})')\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.ylabel('Error (degrees)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(time_indices, errors[:, 1], label='|Predicted - Actual| Theta', color='purple', linewidth=2)\n",
    "        plt.title(f'Theta Absolute Error (Window Size: {window_size})')\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.ylabel('Error (degrees)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'errors_window_{window_size}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'time_indices': time_indices,\n",
    "            'rmse_phi': rmse_phi,\n",
    "            'rmse_theta': rmse_theta\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.title(f'Loss Curves (Window Size: {window_size})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_window_{window_size}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for window_size, result in results.items():\n",
    "        plt.plot(result['val_losses'], label=f'Window {window_size}')\n",
    "    plt.title('Validation Loss Comparison Across Window Sizes')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('window_size_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = os.path.join(\"..\", \"Data\", \"data_angles\", \"data6_angles.csv\")\n",
    "    \n",
    "    print(\"Attempting to access:\", data_path)\n",
    "    print(\"Full path:\", os.path.abspath(data_path))\n",
    "    if os.path.exists(data_path):\n",
    "        print(\"File found!\")\n",
    "    else:\n",
    "        print(\"File not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    window_sizes = [8, 16, 32,64,128]\n",
    "    results = experiment_window_sizes(data_path, window_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19e251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c735fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a43c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
